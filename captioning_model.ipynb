{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2e228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: transformers in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (4.56.1)\n",
      "Requirement already satisfied: huggingface_hub in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (0.34.4)\n",
      "Requirement already satisfied: pickle5 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (0.0.11)\n",
      "Requirement already satisfied: filelock in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from huggingface_hub) (1.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dac/miniconda3/envs/torchenv/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pip install torch torchvision transformers huggingface_hub pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the BLIP model for image captioning from Hugging Face\n",
    "def load_captioning_model():\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    return processor, model\n",
    "\n",
    "# Initialize the model\n",
    "processor, model = load_captioning_model()\n",
    "print(\"Image captioning model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load image from URL or local path\n",
    "def load_image(image_source):\n",
    "    \"\"\"Load image from URL or local file path\"\"\"\n",
    "    try:\n",
    "        if image_source.startswith('http'):\n",
    "            response = requests.get(image_source)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:\n",
    "            image = Image.open(image_source).convert('RGB')\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to generate captions\n",
    "def generate_caption(image_source, beam_size=5, max_length=50):\n",
    "    \"\"\"Generate captions for an image using BLIP model\"\"\"\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = load_image(image_source)\n",
    "        if image is None:\n",
    "            return [\"Error: Could not load image\"]\n",
    "        \n",
    "        # Process the image\n",
    "        inputs = processor(image, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate caption with beam search\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs, \n",
    "                max_length=max_length,\n",
    "                num_beams=beam_size,\n",
    "                num_return_sequences=beam_size,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode the generated captions\n",
    "        captions = []\n",
    "        for i in range(beam_size):\n",
    "            caption = processor.decode(out[i], skip_special_tokens=True)\n",
    "            captions.append(caption)\n",
    "        \n",
    "        return captions\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating caption: {e}\")\n",
    "        return [\"Error generating caption\"]\n",
    "\n",
    "# Function to display image with captions\n",
    "def caption_image(image_source, beam_size=5):\n",
    "    \"\"\"Display image with generated captions\"\"\"\n",
    "    # Load and display image\n",
    "    image = load_image(image_source)\n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    # Generate captions\n",
    "    captions = generate_caption(image_source, beam_size)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title('Input Image')\n",
    "    \n",
    "    # Display captions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis('off')\n",
    "    caption_text = \"Generated Captions:\\n\\n\"\n",
    "    for i, caption in enumerate(captions, 1):\n",
    "        caption_text += f\"{i}. {caption}\\n\\n\"\n",
    "    \n",
    "    plt.text(0.1, 0.9, caption_text, fontsize=12, verticalalignment='top',\n",
    "             transform=plt.gca().transAxes, wrap=True,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with example images\n",
    "test_images = [\n",
    "    \"http://images.cocodataset.org/train2017/000000505539.jpg\",\n",
    "    \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\",  # cat\n",
    "    \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\",    # dog\n",
    "]\n",
    "\n",
    "# Generate captions for test images\n",
    "for i, image_url in enumerate(test_images, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Image {i}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    captions = caption_image(image_url, beam_size=3)\n",
    "    \n",
    "    print(\"\\nGenerated Captions:\")\n",
    "    for j, caption in enumerate(captions, 1):\n",
    "        print(f\"{j}. {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e8c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM implementation for Vision Transformer in BLIP model\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import functional as F\n",
    "import cv2\n",
    "\n",
    "class GradCAMViT:\n",
    "    \"\"\"Grad-CAM implementation for Vision Transformer models like BLIP's vision encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, model, processor, target_layer_name='vision_model.encoder.layers.-1.attention.self'):\n",
    "        \"\"\"\n",
    "        Initialize Grad-CAM for ViT\n",
    "        Args:\n",
    "            model: BLIP model\n",
    "            processor: BLIP processor\n",
    "            target_layer_name: Name of the target layer (last attention layer by default)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.target_layer_name = target_layer_name\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hooks = []\n",
    "        \n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        \"\"\"Hook function to save gradients\"\"\"\n",
    "        self.gradients = grad_output[0]\n",
    "        \n",
    "    def save_activation(self, module, input, output):\n",
    "        \"\"\"Hook function to save activations\"\"\"\n",
    "        self.activations = output\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register forward and backward hooks\"\"\"\n",
    "        # Get the target layer\n",
    "        target_layer = self.get_target_layer()\n",
    "        \n",
    "        # Register hooks\n",
    "        forward_hook = target_layer.register_forward_hook(self.save_activation)\n",
    "        backward_hook = target_layer.register_backward_hook(self.save_gradient)\n",
    "        \n",
    "        self.hooks = [forward_hook, backward_hook]\n",
    "        \n",
    "    def get_target_layer(self):\n",
    "        \"\"\"Get the target layer from the model\"\"\"\n",
    "        # Navigate to the last attention layer in the vision encoder\n",
    "        vision_model = self.model.vision_model\n",
    "        # Get the last transformer layer\n",
    "        last_layer = vision_model.encoder.layers[-1]\n",
    "        # Get the attention mechanism\n",
    "        return last_layer.attention.self\n",
    "        \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def generate_cam(self, image_path, caption_text=None, patch_size=16):\n",
    "        \"\"\"\n",
    "        Generate Grad-CAM for the given image\n",
    "        Args:\n",
    "            image_path: Path to the input image\n",
    "            caption_text: Optional text prompt for conditional generation\n",
    "            patch_size: Size of ViT patches (default 16 for BLIP)\n",
    "        \"\"\"\n",
    "        # Load and preprocess image\n",
    "        image = load_image(image_path)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "            \n",
    "        inputs = self.processor(image, text=caption_text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Register hooks\n",
    "        self.register_hooks()\n",
    "        \n",
    "        try:\n",
    "            # Forward pass\n",
    "            self.model.eval()\n",
    "            \n",
    "            if caption_text is None:\n",
    "                # For image captioning, we need to do generation\n",
    "                outputs = self.model.generate(**inputs, max_length=50, num_beams=1, return_dict_in_generate=True, output_scores=True)\n",
    "                # Use the last generated token's score for backpropagation\n",
    "                target_score = outputs.scores[-1].max()\n",
    "            else:\n",
    "                # For conditional generation with text\n",
    "                outputs = self.model(**inputs)\n",
    "                target_score = outputs.logits.max()\n",
    "            \n",
    "            # Backward pass\n",
    "            self.model.zero_grad()\n",
    "            target_score.backward(retain_graph=True)\n",
    "            \n",
    "            # Generate CAM\n",
    "            if self.gradients is not None and self.activations is not None:\n",
    "                # Get gradients and activations\n",
    "                gradients = self.gradients.cpu().data.numpy()[0]  # [seq_len, hidden_dim]\n",
    "                activations = self.activations.cpu().data.numpy()[0]  # [seq_len, hidden_dim]\n",
    "                \n",
    "                # Remove CLS token (first token)\n",
    "                gradients = gradients[1:]  # Remove CLS token\n",
    "                activations = activations[1:]  # Remove CLS token\n",
    "                \n",
    "                # Calculate weights (global average pooling of gradients)\n",
    "                weights = np.mean(gradients, axis=1)  # [seq_len-1]\n",
    "                \n",
    "                # Generate CAM\n",
    "                cam = np.zeros(activations.shape[0])  # [seq_len-1]\n",
    "                for i, w in enumerate(weights):\n",
    "                    cam[i] = w * np.mean(activations[i])\n",
    "                \n",
    "                # Normalize CAM\n",
    "                cam = np.maximum(cam, 0)  # ReLU\n",
    "                if cam.max() > 0:\n",
    "                    cam = cam / cam.max()\n",
    "                \n",
    "                # Reshape to spatial dimensions\n",
    "                # For ViT, we need to convert from sequence to spatial\n",
    "                num_patches = int(np.sqrt(len(cam)))\n",
    "                if num_patches * num_patches == len(cam):\n",
    "                    cam_2d = cam.reshape(num_patches, num_patches)\n",
    "                else:\n",
    "                    # Handle cases where patches don't form perfect square\n",
    "                    side = int(np.sqrt(len(cam)))\n",
    "                    cam_2d = cam[:side*side].reshape(side, side)\n",
    "                \n",
    "                return cam_2d, image\n",
    "            \n",
    "        finally:\n",
    "            self.remove_hooks()\n",
    "        \n",
    "        return None, image\n",
    "\n",
    "# Visualization functions\n",
    "def overlay_cam_on_image(image, cam, alpha=0.6, colormap=cv2.COLORMAP_JET):\n",
    "    \"\"\"\n",
    "    Overlay CAM heatmap on the original image\n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        cam: 2D numpy array representing the CAM\n",
    "        alpha: Transparency factor\n",
    "        colormap: OpenCV colormap\n",
    "    \"\"\"\n",
    "    # Convert PIL image to numpy array\n",
    "    img_array = np.array(image)\n",
    "    height, width = img_array.shape[:2]\n",
    "    \n",
    "    # Resize CAM to match image dimensions\n",
    "    cam_resized = cv2.resize(cam, (width, height))\n",
    "    \n",
    "    # Normalize CAM to 0-255\n",
    "    cam_normalized = np.uint8(255 * cam_resized)\n",
    "    \n",
    "    # Apply colormap\n",
    "    heatmap = cv2.applyColorMap(cam_normalized, colormap)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Overlay heatmap on original image\n",
    "    overlayed = img_array * (1 - alpha) + heatmap * alpha\n",
    "    overlayed = np.uint8(overlayed)\n",
    "    \n",
    "    return overlayed\n",
    "\n",
    "def visualize_gradcam_results(image_path, gradcam_vit, caption_text=None):\n",
    "    \"\"\"\n",
    "    Complete visualization pipeline for Grad-CAM results\n",
    "    \"\"\"\n",
    "    # Generate CAM\n",
    "    cam, original_image = gradcam_vit.generate_cam(image_path, caption_text)\n",
    "    \n",
    "    if cam is None:\n",
    "        print(\"Failed to generate CAM\")\n",
    "        return\n",
    "    \n",
    "    # Generate caption for the image\n",
    "    captions = generate_caption(image_path, beam_size=1)\n",
    "    main_caption = captions[0] if captions else \"No caption generated\"\n",
    "    \n",
    "    # Create overlay\n",
    "    overlayed_image = overlay_cam_on_image(original_image, cam)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # CAM heatmap\n",
    "    im1 = axes[1].imshow(cam, cmap='jet')\n",
    "    axes[1].set_title('Grad-CAM Heatmap')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(overlayed_image)\n",
    "    axes[2].set_title('Grad-CAM Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Caption\n",
    "    axes[3].axis('off')\n",
    "    caption_text_display = f\"Generated Caption:\\n\\n{main_caption}\"\n",
    "    axes[3].text(0.1, 0.7, caption_text_display, fontsize=12, \n",
    "                verticalalignment='top', wrap=True,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "    axes[3].set_title('Model Prediction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cam, overlayed_image, main_caption\n",
    "\n",
    "print(\"Grad-CAM for ViT implementation ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df801176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Grad-CAM Analysis Functions\n",
    "\n",
    "def compare_attention_layers(image_path, layer_indices=[-1, -2, -3]):\n",
    "    \"\"\"\n",
    "    Compare Grad-CAM across different transformer layers\n",
    "    \"\"\"\n",
    "    original_image = load_image(image_path)\n",
    "    if original_image is None:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(layer_indices) + 1, figsize=(5 * (len(layer_indices) + 1), 10))\n",
    "    \n",
    "    # Show original image\n",
    "    axes[0, 0].imshow(original_image)\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    for i, layer_idx in enumerate(layer_indices):\n",
    "        # Create Grad-CAM for specific layer\n",
    "        gradcam_layer = GradCAMViT(model, processor)\n",
    "        \n",
    "        # Modify target layer\n",
    "        try:\n",
    "            vision_model = model.vision_model\n",
    "            target_layer = vision_model.encoder.layers[layer_idx].attention.self\n",
    "            \n",
    "            # Generate CAM for this layer\n",
    "            cam, _ = gradcam_layer.generate_cam(image_path)\n",
    "            \n",
    "            if cam is not None:\n",
    "                # Show heatmap\n",
    "                im1 = axes[0, i + 1].imshow(cam, cmap='jet')\n",
    "                axes[0, i + 1].set_title(f'Layer {layer_idx} CAM')\n",
    "                axes[0, i + 1].axis('off')\n",
    "                plt.colorbar(im1, ax=axes[0, i + 1], fraction=0.046, pad=0.04)\n",
    "                \n",
    "                # Show overlay\n",
    "                overlay = overlay_cam_on_image(original_image, cam)\n",
    "                axes[1, i + 1].imshow(overlay)\n",
    "                axes[1, i + 1].set_title(f'Layer {layer_idx} Overlay')\n",
    "                axes[1, i + 1].axis('off')\n",
    "            else:\n",
    "                axes[0, i + 1].text(0.5, 0.5, 'Failed to\\ngenerate CAM', \n",
    "                                   ha='center', va='center', transform=axes[0, i + 1].transAxes)\n",
    "                axes[0, i + 1].axis('off')\n",
    "                axes[1, i + 1].axis('off')\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with layer {layer_idx}: {e}\")\n",
    "            axes[0, i + 1].text(0.5, 0.5, f'Error:\\n{str(e)[:20]}...', \n",
    "                               ha='center', va='center', transform=axes[0, i + 1].transAxes)\n",
    "            axes[0, i + 1].axis('off')\n",
    "            axes[1, i + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_attention_patterns(image_path, save_results=False):\n",
    "    \"\"\"\n",
    "    Detailed analysis of attention patterns\n",
    "    \"\"\"\n",
    "    print(f\"🔬 Detailed Attention Analysis\")\n",
    "    print(f\"Image: {os.path.basename(image_path) if image_path.startswith('/') else 'Online Image'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Generate caption and CAM\n",
    "    captions = generate_caption(image_path, beam_size=5)\n",
    "    cam, original_image = gradcam_vit.generate_cam(image_path)\n",
    "    \n",
    "    if cam is None:\n",
    "        print(\"❌ Failed to generate attention map\")\n",
    "        return\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"📊 Attention Statistics:\")\n",
    "    print(f\"   • Shape: {cam.shape}\")\n",
    "    print(f\"   • Min activation: {cam.min():.4f}\")\n",
    "    print(f\"   • Max activation: {cam.max():.4f}\")\n",
    "    print(f\"   • Mean activation: {cam.mean():.4f}\")\n",
    "    print(f\"   • Std activation: {cam.std():.4f}\")\n",
    "    \n",
    "    # Find most attended regions\n",
    "    flat_cam = cam.flatten()\n",
    "    top_indices = np.argsort(flat_cam)[-5:]  # Top 5 patches\n",
    "    print(f\"\\n🎯 Top 5 Most Attended Patches:\")\n",
    "    for i, idx in enumerate(reversed(top_indices)):\n",
    "        row, col = divmod(idx, cam.shape[1])\n",
    "        activation = flat_cam[idx]\n",
    "        print(f\"   {i+1}. Patch ({row}, {col}): {activation:.4f}\")\n",
    "    \n",
    "    # Generated captions\n",
    "    print(f\"\\n📝 Generated Captions:\")\n",
    "    for i, caption in enumerate(captions, 1):\n",
    "        print(f\"   {i}. {caption}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(original_image)\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # CAM heatmap\n",
    "    im1 = axes[0, 1].imshow(cam, cmap='jet')\n",
    "    axes[0, 1].set_title('Attention Heatmap')\n",
    "    axes[0, 1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0, 1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = overlay_cam_on_image(original_image, cam)\n",
    "    axes[0, 2].imshow(overlay)\n",
    "    axes[0, 2].set_title('Attention Overlay')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Thresholded attention (top 25%)\n",
    "    threshold = np.percentile(cam, 75)\n",
    "    cam_thresh = np.where(cam >= threshold, cam, 0)\n",
    "    axes[1, 0].imshow(cam_thresh, cmap='jet')\n",
    "    axes[1, 0].set_title(f'Top 25% Attention\\n(>{threshold:.3f})')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Attention distribution histogram\n",
    "    axes[1, 1].hist(cam.flatten(), bins=50, alpha=0.7, color='blue')\n",
    "    axes[1, 1].axvline(cam.mean(), color='red', linestyle='--', label=f'Mean: {cam.mean():.3f}')\n",
    "    axes[1, 1].axvline(threshold, color='orange', linestyle='--', label=f'75th percentile: {threshold:.3f}')\n",
    "    axes[1, 1].set_title('Attention Distribution')\n",
    "    axes[1, 1].set_xlabel('Activation Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # Captions\n",
    "    axes[1, 2].axis('off')\n",
    "    caption_text = \"Generated Captions:\\n\\n\"\n",
    "    for i, caption in enumerate(captions[:3], 1):\n",
    "        caption_text += f\"{i}. {caption}\\n\\n\"\n",
    "    \n",
    "    axes[1, 2].text(0.1, 0.9, caption_text, fontsize=10, \n",
    "                   verticalalignment='top', transform=axes[1, 2].transAxes,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgreen\", alpha=0.8))\n",
    "    axes[1, 2].set_title('Model Predictions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if save_results:\n",
    "        # Save the overlay image\n",
    "        output_path = f\"gradcam_result_{int(time.time())}.png\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_image)\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(overlay)\n",
    "        plt.title('Grad-CAM')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"💾 Results saved to: {output_path}\")\n",
    "    \n",
    "    return cam, overlay, captions\n",
    "\n",
    "print(\"🚀 Advanced analysis functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b6acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
